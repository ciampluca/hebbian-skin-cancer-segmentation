# @package optim

resume: false

optimizer:
    _target_: torch.optim.Adam
    lr: 0.01

lr_scheduler:
    _target_: torch.optim.lr_scheduler.MultiStepLR
    gamma: 0.1
    milestones: [50, 75, 100]

batch_size: 8
batch_accumulation: 1
epochs: 100

val_freq: 1
val_batch_size: ${.batch_size}
val_device: cuda

num_workers: 16

debug: true
debug_freq: 1
log_every: 1
save_debug_val_images: true
save_debug_train_images: true

loss:
    _target_: torch.nn.BCEWithLogitsLoss

entropy_lambda: 0   # used by EM; can be a fixed value or 'adaptive'
starting_entropy_lambda: 5      # used only with 'adaptive' EM

wavelet_lambda: 0   # used by XNet; can be a fixed value or 'adaptive'
starting_wavelet_lambda: 5      # used only with 'adaptive' XNet
use_h_wavelet: false     # if false, it uses low frequency wavelet

perturbation: 0     # used by CCT; number of performed input perturbations
uniform_range: 0.3
perturbation_lambda: 0
starting_perturbation_lambda: 5     # can be a fixed value or 'adaptive'

teacher_lambda: 0   # used by MT; can be a fixed value or 'adaptive'
starting_teacher_lambda: 5
teacher_alpha: 0.99

superpix_lambda: 0   # used by superpix; can be a fixed value or 'adaptive'
starting_superpix_lambda: 5

diffusion_timestamp: 0     # used by ddpm

